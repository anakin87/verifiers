model = "willcb/Qwen3-0.6B"

[env]
id = "anakin87/tictactoe"

[env.args]
optimal_opponent_prob = 0.5
use_think = true
num_examples = 1000

[inference]
gpus = 1

[inference.args]
enforce_eager = true

[trainer]
gpus = 1

[trainer.args]
run_name = "tictactoe"

# Hub configuration
push_to_hub = true
hub_model_id = "anakin87/qwen3-ttt"
hub_strategy = "every_save"
hub_private_repo = true
save_strategy = "steps"
save_steps = 10
save_total_limit = 1

# Batch configuration:
# - rollouts_per_example: completions generated per prompt (group size for advantage normalization)
# - micro_batch_size: rollouts processed per GPU per gradient accumulation step
# - batch_size: total rollouts per training step (must be divisible by micro_batch_size and rollouts_per_example)
micro_batch_size = 1
rollouts_per_example = 8
batch_size = 64
lora_rank = 4

# Training schedule
max_steps = 1000
# Sequence length: max tokens for full conversation (prompt + all turns)
max_tokens = 512
max_seq_len = 2048
