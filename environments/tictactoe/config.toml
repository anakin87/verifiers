model = "willcb/Qwen3-0.6B"

[env]
id = "anakin87/tictactoe"

[env.args]
optimal_opponent_prob = 0.5
use_think = true
num_examples = 1000

[inference]
gpus = 1

[inference.args]
enforce_eager = true

[trainer]
gpus = 1

[trainer.args]
run_name = "tictactoe"

# Batch configuration:
# - rollouts_per_example: completions generated per prompt (group size)
# - micro_batch_size: rollouts processed per GPU per gradient accumulation step
# - batch_size: total rollouts per training step (must be divisible by micro_batch_size and rollouts_per_example)
micro_batch_size = 1
rollouts_per_example = 8
batch_size = 64

lora_rank = 4

max_steps = 1000

max_tokens = 512
max_seq_len = 2048 # Sequence length: max tokens for full conversation (prompt + all turns)

# Hub configuration
# (https://huggingface.co/docs/transformers/v4.57.3/en/main_classes/trainer#transformers.TrainingArguments)
push_to_hub = true
hub_model_id = "YOUR-MODEL-ID"
hub_strategy = "every_save"
save_strategy = "steps"
save_steps = 10
save_total_limit = 1
